{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pymysql\n",
    "import re\n",
    "from langgraph.graph import END\n",
    "from typing import List,TypedDict, Dict, Any, Optional\n",
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain import hub\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:asyncio:Using selector: SelectSelector\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "tracing = os.getenv(\"LANGSMITH_TRACING\")\n",
    "api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "Mistral_api_key = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "# Chemin vers le r√©pertoire contenant les bases de connaissances\n",
    "DATA_ROOT = \"../data\"\n",
    "ECHANGES_PATH = os.path.join(DATA_ROOT, \"echanges\")\n",
    "REGLES_PATH = os.path.join(DATA_ROOT, \"regles\") \n",
    "OFFICIAL_DOCS_PATH = os.path.join(DATA_ROOT,\"docs_officiels\")\n",
    "\n",
    "# Chat model mistral\n",
    "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\nif __name__ == \"__main__\":\\n    try:\\n        db_manager = DatabaseManager()\\n        if db_manager.tester_connexion():\\n            resultats = db_manager.rechercher_dossier(numero_dossier=\\'82-2069\\')\\n            print(\"\\nüìã R√©sultats de la recherche :\")\\n            for dossier in resultats:\\n                print(dossier)\\n        else:\\n            print(\" √âchec de la connexion.\")\\n    except Exception as e:\\n        print(f\" Erreur critique : {e}\")\\n        traceback.print_exc()\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DatabaseManager:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.config = {\n",
    "            'user': os.getenv('SQL_USER'),\n",
    "            'password': os.getenv('SQL_PASSWORD', ''),\n",
    "            'host': os.getenv('SQL_HOST', 'localhost'),\n",
    "            'database': os.getenv('SQL_DB'),\n",
    "            'port': int(os.getenv('SQL_PORT', '3306'))\n",
    "        }\n",
    "        if not all([self.config['user'], self.config['host'], self.config['database']]):\n",
    "            raise ValueError(\"Variables essentielles manquantes : SQL_USER, SQL_HOST ou SQL_DB.\")\n",
    "        \n",
    "    \n",
    "    def tester_connexion(self) -> bool:\n",
    "        try:\n",
    "            conn = pymysql.connect(**self.config)\n",
    "            curseur = conn.cursor()\n",
    "            curseur.execute(\"SELECT 1\")\n",
    "            curseur.fetchone()\n",
    "            curseur.close()\n",
    "            conn.close()\n",
    "            print(\" Connexion r√©ussie avec pymysql.\")\n",
    "            return True\n",
    "        except pymysql.Error as erreur:\n",
    "            print(f\" √âchec de la connexion : {erreur}\")\n",
    "            return False\n",
    "    \n",
    "    def rechercher_dossier(self, numero_dossier: Optional[str] = None, **kwargs) -> List[Dict[str, Any]]:\n",
    "        try:\n",
    "            conn = pymysql.connect(**self.config)\n",
    "            curseur = conn.cursor(pymysql.cursors.DictCursor)  # Pour des r√©sultats sous forme de dictionnaires\n",
    "            conditions = []\n",
    "            parametres = []\n",
    "\n",
    "            if numero_dossier:\n",
    "                conditions.append(\"Numero = %s\")\n",
    "                parametres.append(numero_dossier)\n",
    "\n",
    "            for cle, valeur in kwargs.items():\n",
    "                if valeur is not None:\n",
    "                    conditions.append(f\"{cle} = %s\")\n",
    "                    parametres.append(valeur)\n",
    "\n",
    "            requete = f\"SELECT * FROM dossiers WHERE {' AND '.join(conditions) if conditions else '1=1'}\"\n",
    "            curseur.execute(requete, parametres)\n",
    "            resultats = curseur.fetchall()\n",
    "            curseur.close()\n",
    "            conn.close()\n",
    "            return resultats\n",
    "        except pymysql.Error as erreur:\n",
    "            print(f\" Erreur de recherche : {erreur}\")\n",
    "            return []\n",
    "\"\"\"  \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        db_manager = DatabaseManager()\n",
    "        if db_manager.tester_connexion():\n",
    "            resultats = db_manager.rechercher_dossier(numero_dossier='82-2069')\n",
    "            print(\"\\nüìã R√©sultats de la recherche :\")\n",
    "            for dossier in resultats:\n",
    "                print(dossier)\n",
    "        else:\n",
    "            print(\" √âchec de la connexion.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur critique : {e}\")\n",
    "        traceback.print_exc()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire le num√©ro de dossier de la question\n",
    "def extract_dossier_number(question: str) -> List[str]:\n",
    "    # Pattern pour rechercher les num√©ros de dossier (format XX-XXXX ou similaire)\n",
    "    patterns = [\n",
    "        r'\\b\\d{2}-\\d{4}\\b',  # Format 82-2069\n",
    "        r'\\bdossier\\s+(?:n¬∞|num√©ro|numero|n|¬∞|)?\\s*(\\w+-\\w+)\\b',  # Format avec \"dossier n¬∞ XXX-XXX\"\n",
    "        r'\\bdossier\\s+(?:n¬∞|num√©ro|numero|n|¬∞|)?\\s*(\\d+)\\b'  # Format avec \"dossier n¬∞ XXX\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, question, re.IGNORECASE)\n",
    "        if matches:\n",
    "            results.extend(matches)\n",
    "    \n",
    "    return list(set(results))  # √âliminer les doublons\n",
    "    \n",
    "   \n",
    "\n",
    "# Fonction pour rechercher les informations d'un dossier dans la base de donn√©es via DatabaseManager\n",
    "def search_dossier_in_db(dossier_numbers: List[str]) -> List[Dict[str, Any]]:\n",
    "    try:\n",
    "        db_manager = DatabaseManager()\n",
    "        \n",
    "        # Tester la connexion avant de proc√©der\n",
    "        if not db_manager.tester_connexion():\n",
    "            print(\"Impossible de se connecter √† la base de donn√©es\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for num in dossier_numbers:\n",
    "            dossier_results = db_manager.rechercher_dossier(numero_dossier=num)\n",
    "            results.extend(dossier_results)\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la recherche dans la base de donn√©es: {e}\")\n",
    "        return []\n",
    "\n",
    "        \n",
    "def db_resultats_to_documents(resultats: List[Dict[str,Any]]) -> List[Document]:\n",
    "    documents = [] \n",
    "    for resultat in resultats:\n",
    "        #contenu format√© √† partir des donne du dossier\n",
    "        content = f\"\"\"\n",
    "        - Informations sur le dossier {resultat.get('Numero', 'N/A')}:\n",
    "        - Nom de l'usager: {resultat.get('nom_usager', 'N/A')}\n",
    "        - Date de cr√©ation: {resultat.get('date_creation', 'N/A')}\n",
    "        - Derni√®re modification: {resultat.get('derniere_modification', 'N/A')}\n",
    "        - Agent affect√©: {resultat.get('agent_affecter', 'N/A')}\n",
    "        - Instructeur: {resultat.get('instructeur', 'N/A')}\n",
    "        - Statut actuel: {resultat.get('statut', 'N/A')}\n",
    "        - Statut visible par l'usager: {resultat.get('statut_visible_usager', 'N/A')}\n",
    "        - Montant: {resultat.get('montant', 'N/A')} ‚Ç¨\n",
    "        \"\"\"\n",
    "        \n",
    "        #cr√©ation d'un document Langchain avec le meta data\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": \"base_de_donnees\",\n",
    "                \"type\":\"dossier\",\n",
    "                \"num√©ro\": resultat.get('Numero','N/A'),\n",
    "                \"section\":\"Informations dossier\",\n",
    "                \"page\": \"1\", \n",
    "                \"update_date\": resultat.get('derniere_modification', 'N/A')\n",
    "                \n",
    "            }\n",
    "        )\n",
    "        \n",
    "        documents.append(doc)\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange1.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange10.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange11.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange12.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange13.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange14.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange15.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange16.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange17.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange18.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange19.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange2.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange20.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange21.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange22.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange23.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange24.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange25.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange26.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange27.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange28.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange29.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange3.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange30.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange31.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange32.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange33.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange34.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange35.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange36.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange37.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange38.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange39.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange4.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange40.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange41.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange42.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange43.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange44.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange45.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange46.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange5.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange6.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange7.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange8.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\echanges\\echange9.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\regles\\Regles_RAG_partie_1.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\regles\\Regles_RAG_partie_2.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\docs_officiels\\FAQ_kap_numerique.txt\n",
      "DEBUG:langchain_community.document_loaders.directory:Processing file: ..\\data\\docs_officiels\\Fiche_action_1.2.5_kap_numerique.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 documents d'echanges charg√©s.\n",
      "2 documents de regles charg√©s.\n",
      "2 documents de docs_officiels charg√©s.\n",
      "Total : 50 documents charg√©s dans la base de connaissances\n"
     ]
    }
   ],
   "source": [
    "#Fonction pour charger tous les documents des diff√©rents dossiers\n",
    "def load_all_documents():\n",
    "    all_docs = []\n",
    "    \n",
    "    # On va d'abord charger les documents des echanges \n",
    "    try:\n",
    "        echanges_loader = DirectoryLoader(\n",
    "            ECHANGES_PATH,\n",
    "            glob=\"**/*txt\",  # Charger tous les .txt, y compris dans les sous-dossiers\n",
    "            loader_cls=TextLoader,\n",
    "            loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "            recursive=True  # Charger r√©cursivement dans les sous-dossiers\n",
    "        )\n",
    "        echanges_docs = echanges_loader.load()\n",
    "        for doc in echanges_docs:\n",
    "            doc.metadata[\"category\"] = \"echanges\"\n",
    "        print(f\"{len(echanges_docs)} documents d'echanges charg√©s.\")\n",
    "        all_docs.extend(echanges_docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des dossiers d'echanges:{e}\")\n",
    "        \n",
    "    #Ici on va charger les documents des regles\n",
    "    try:\n",
    "        regles_loader = DirectoryLoader(\n",
    "            REGLES_PATH,\n",
    "            glob=\"*.txt\",\n",
    "            loader_cls=TextLoader,\n",
    "            loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "        )\n",
    "        \n",
    "        regles_docs = regles_loader.load()\n",
    "        for doc in regles_docs:\n",
    "            doc.metadata[\"category\"] = \"regles\"\n",
    "        print(f\"{len(regles_docs)} documents de regles charg√©s.\")\n",
    "        all_docs.extend(regles_docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du changement des documents de regles: {e}\")\n",
    "        \n",
    "    #Ici on va charger les documents des docs_officiels\n",
    "    try:\n",
    "        official_docs_loader = DirectoryLoader(\n",
    "            OFFICIAL_DOCS_PATH,\n",
    "            glob=\"*.txt\",\n",
    "            loader_cls=TextLoader,\n",
    "            loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "        ) \n",
    "        official_docs = official_docs_loader.load()\n",
    "        for doc in official_docs:\n",
    "            doc.metadata[\"category\"] = \"docs_officiels\"\n",
    "        print(f\"{len(official_docs)} documents de docs_officiels charg√©s.\")\n",
    "        all_docs.extend(official_docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des documetns officiels: {e}\")\n",
    "    \n",
    "    print(f\"Total : {len(all_docs)} documents charg√©s dans la base de connaissances\")\n",
    "    return all_docs\n",
    "            \n",
    "docs = load_all_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©couper les documents en chunks pour un meilleur traitement par le mod√®le\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.json HTTP/1.1\" 401 0\n",
      "c:\\Python312\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les embeddings Mistral\n",
    "embeddings = MistralAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A045DA9F0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A02D16550> server_hostname='api.mistral.ai' timeout=120\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A047F78C0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:29:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimitbysize-remaining-minute', b'19996618'), (b'x-ratelimitbysize-remaining-month', b'199999949396'), (b'ratelimitbysize-query-cost', b'3382'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19996618'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'ratelimitbysize-reset', b'54'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-envoy-upstream-service-time', b'210'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'211'), (b'x-kong-proxy-latency', b'6'), (b'x-kong-request-id', b'087a03684733a57313421130e2d2af53'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ftSVZJchw4NGXDwwKsXPQdYPNGHatAlkwekgz.0biUU-1743762546-1.0.1.1-t9EAWHYQ6fkRAcSDtOcrVxt5UN2xEEAZ3cugn4wa3YuKYHAqLVlfIbq6YWu.E8cqa5PcU8C_pfUSc5o9VVUM0iPr9tqtgw.zCB815bgIiIY; path=/; expires=Fri, 04-Apr-25 10:59:06 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b01f668ff3e55d-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Fri, 04 Apr 2025 10:29:07 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'42'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-query-cost', b'3286'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-ratelimitbysize-remaining-minute', b'19993332'), (b'x-ratelimitbysize-remaining-month', b'199999946110'), (b'ratelimitbysize-reset', b'53'), (b'ratelimitbysize-remaining', b'19993332'), (b'ratelimitbysize-limit', b'20000000'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'access-control-allow-origin', b'*'), (b'x-kong-response-latency', b'8'), (b'x-kong-request-id', b'8b37627b03c5383baed7d174bcba1b21'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b01f73fcefe55d-EWR'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A047F7B90>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A02D16550> server_hostname='api.mistral.ai' timeout=120\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A02CC7BF0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:29:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimitbysize-remaining-minute', b'19990046'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'ratelimitbysize-query-cost', b'3286'), (b'x-ratelimitbysize-remaining-month', b'199999942824'), (b'ratelimitbysize-remaining', b'19990046'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-reset', b'21'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-envoy-upstream-service-time', b'263'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'264'), (b'x-kong-proxy-latency', b'13'), (b'x-kong-request-id', b'24961726badb3b30ca16630f75a4f960'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b020373a81f834-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Fri, 04 Apr 2025 10:29:40 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'42'), (b'Connection', b'keep-alive'), (b'x-ratelimitbysize-remaining-minute', b'19986683'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'ratelimitbysize-query-cost', b'3363'), (b'x-ratelimitbysize-remaining-month', b'199999939461'), (b'ratelimitbysize-remaining', b'19986683'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-reset', b'20'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'access-control-allow-origin', b'*'), (b'x-kong-response-latency', b'7'), (b'x-kong-request-id', b'e5c40b1ea259eb56b4daf874c9743661'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b02041c8f7f834-EWR'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A02ED33E0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A02D16550> server_hostname='api.mistral.ai' timeout=120\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A02B5A3F0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:30:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-query-cost', b'3363'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19986086'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'x-ratelimitbysize-remaining-minute', b'19986086'), (b'ratelimitbysize-reset', b'48'), (b'x-ratelimitbysize-remaining-month', b'199999936098'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-envoy-upstream-service-time', b'280'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'282'), (b'x-kong-proxy-latency', b'17'), (b'x-kong-request-id', b'569a9562c75fb36b86bd895bd799dd2a'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b02105bb79f5f7-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:30:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-query-cost', b'3336'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19983203'), (b'x-ratelimitbysize-remaining-month', b'199999932762'), (b'ratelimitbysize-reset', b'46'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-ratelimitbysize-remaining-minute', b'19983203'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'x-envoy-upstream-service-time', b'254'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'255'), (b'x-kong-proxy-latency', b'14'), (b'x-kong-request-id', b'b9c45e5ffb545b175f51ac5ee6dd923e'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b021142cc3f5f7-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:30:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-reset', b'45'), (b'x-ratelimitbysize-remaining-month', b'199999929207'), (b'ratelimitbysize-query-cost', b'3555'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19979872'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'x-ratelimitbysize-remaining-minute', b'19979872'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-envoy-upstream-service-time', b'278'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'279'), (b'x-kong-proxy-latency', b'15'), (b'x-kong-request-id', b'a3dfe34ff775666e484fee0c5348790f'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b0211a79d3f5f7-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:30:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-query-cost', b'3343'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-ratelimitbysize-remaining-minute', b'19980298'), (b'x-ratelimitbysize-remaining-month', b'199999929419'), (b'ratelimitbysize-reset', b'44'), (b'ratelimitbysize-remaining', b'19980298'), (b'ratelimitbysize-limit', b'20000000'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'x-envoy-upstream-service-time', b'251'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'252'), (b'x-kong-proxy-latency', b'14'), (b'x-kong-request-id', b'cff42b6961845760fcf28b51d8708452'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b021208e0ef5f7-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Fri, 04 Apr 2025 10:30:17 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'42'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-query-cost', b'2177'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-ratelimitbysize-remaining-minute', b'19974774'), (b'x-ratelimitbysize-remaining-month', b'199999923687'), (b'ratelimitbysize-reset', b'43'), (b'ratelimitbysize-remaining', b'19974774'), (b'ratelimitbysize-limit', b'20000000'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'access-control-allow-origin', b'*'), (b'x-kong-response-latency', b'14'), (b'x-kong-request-id', b'd6ae974ab2a170450183752e44e4e069'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b0212659aff5f7-EWR'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A02EC8CE0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A02D16550> server_hostname='api.mistral.ai' timeout=120\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A04850C80>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:30:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimitbysize-remaining-minute', b'19979600'), (b'x-ratelimitbysize-remaining-month', b'199999921510'), (b'ratelimitbysize-query-cost', b'2177'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19979600'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'ratelimitbysize-reset', b'12'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-envoy-upstream-service-time', b'214'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'215'), (b'x-kong-proxy-latency', b'6'), (b'x-kong-request-id', b'0dd8a54451afb50430f75546f93e5c0c'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b021e9adb3c436-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:30:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-ratelimitbysize-remaining-minute', b'19976423'), (b'x-ratelimitbysize-remaining-month', b'199999918031'), (b'ratelimitbysize-query-cost', b'3479'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19976423'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'ratelimitbysize-reset', b'10'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-envoy-upstream-service-time', b'255'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'256'), (b'x-kong-proxy-latency', b'13'), (b'x-kong-request-id', b'a2896e714f2482316121f612dda80c5d'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b021f42b9cc436-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Fri, 04 Apr 2025 10:30:51 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'42'), (b'Connection', b'keep-alive'), (b'x-ratelimitbysize-remaining-minute', b'19974121'), (b'x-ratelimitbysize-remaining-month', b'199999915410'), (b'ratelimitbysize-query-cost', b'2621'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19974121'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'ratelimitbysize-reset', b'9'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'access-control-allow-origin', b'*'), (b'x-kong-response-latency', b'10'), (b'x-kong-request-id', b'8e952a1d8d74350490b1c439ca82b64b'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b021fd18a7c436-EWR'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A04791820>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A02D16550> server_hostname='api.mistral.ai' timeout=120\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A04855550>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:31:23 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-query-cost', b'2621'), (b'ratelimitbysize-limit', b'20000000'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'x-ratelimitbysize-remaining-minute', b'19984169'), (b'ratelimitbysize-reset', b'37'), (b'ratelimitbysize-remaining', b'19984169'), (b'x-ratelimitbysize-remaining-month', b'199999915410'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-envoy-upstream-service-time', b'192'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'192'), (b'x-kong-proxy-latency', b'11'), (b'x-kong-request-id', b'8a548d6b0793945f6a5b3f35f2283122'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b022c068eb4308-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er le vectorstore en m√©moire √† partir des chunks et des embeddings\n",
    "vector_store = InMemoryVectorStore.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.smith.langchain.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (2): api.smith.langchain.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"GET /info HTTP/1.1\" 200 672\n",
      "DEBUG:urllib3.connectionpool:https://api.smith.langchain.com:443 \"GET /commits/rlm/rag-prompt/latest HTTP/1.1\" 200 844\n"
     ]
    }
   ],
   "source": [
    "# D√©finir le prompt pour la recherche de r√©ponse\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# D√©finition de l'√©tat de l'application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]  # Liste d'objets Document\n",
    "    db_results: List[Dict[str,Any]] # R√©sultats de la base de donn√©es\n",
    "    answer: str\n",
    "\n",
    "# Fonction de r√©cup√©ration des donn√©es de la base de donn√©es\n",
    "def search_database(state: State) -> Dict[str, Any]:\n",
    "    # Extraire les num√©ros de dossier de la question\n",
    "    dossier_numbers = extract_dossier_number(state[\"question\"])\n",
    "    \n",
    "    # Si des num√©ros de dossier sont trouv√©s, rechercher dans la base de donn√©es\n",
    "    db_results = []\n",
    "    if dossier_numbers:\n",
    "        db_results = search_dossier_in_db(dossier_numbers)\n",
    "    \n",
    "    return {\"db_results\": db_results}\n",
    " \n",
    "    \n",
    "# Fonction de r√©cup√©ration (retrieval) des documents pertinents\n",
    "def retrieve(state: State) -> Dict[str, Any]:\n",
    "    try:\n",
    "        # R√©cup√©rer les 3 documents les plus pertinents depuis le vectorstore\n",
    "        retrieved_docs = vector_store.similarity_search(state[\"question\"], k=5)\n",
    "        \n",
    "        # Convertir les r√©sultats de la base de donn√©es en documents\n",
    "        db_docs = db_resultats_to_documents(state[\"db_results\"])\n",
    "        \n",
    "        # Combiner les documents de la base de donn√©es et les documents du vectorstore\n",
    "        combined_docs = db_docs + retrieved_docs\n",
    "        \n",
    "        return {\"context\": combined_docs}\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur dans la fonction retrieve: {e}\")\n",
    "        return {\"context\": []}  # Retourner une liste vide en cas d'erreur\n",
    "\n",
    "\n",
    "# Fonction de g√©n√©ration de la r√©ponse\n",
    "def generate(state: State) -> Dict[str, Any]:\n",
    "    try:\n",
    "        # V√©rifier si le contexte est vide\n",
    "        if not state[\"context\"]:\n",
    "            return {\"answer\": \"Je n'ai pas trouv√© d'informations pertinentes pour r√©pondre √† votre question.\"}\n",
    "        \n",
    "        # Agr√©gation du contenu des documents r√©cup√©r√©s avec d√©tails sur les sources\n",
    "        docs_details = []\n",
    "        for doc in state[\"context\"]:  \n",
    "            source = doc.metadata.get(\"source\", \"Source inconnue\")\n",
    "            category = doc.metadata.get(\"category\", \"non classifi√©\")\n",
    "            \n",
    "            # Traitement sp√©cial pour les documents de la base de donn√©es\n",
    "            if source == \"base_de_donnees\":\n",
    "                file_name = f\"Base de donn√©es - Dossier {doc.metadata.get('num√©ro', 'inconnu')}\"\n",
    "                section = doc.metadata.get(\"section\", \"Section non sp√©cifi√©e\")\n",
    "                page = \"N/A\"\n",
    "                update_date = doc.metadata.get(\"update_date\", \"Date non disponible\")\n",
    "            else:\n",
    "                file_name = os.path.basename(source)\n",
    "                section = doc.metadata.get(\"section\", \"Section non sp√©cifi√©e\")\n",
    "                page = doc.metadata.get(\"page\", \"Page non sp√©cifi√©e\")\n",
    "                update_date = doc.metadata.get(\"update_date\", \"Date non disponible\")\n",
    "\n",
    "            docs_details.append({\n",
    "                \"content\": doc.page_content,\n",
    "                \"file_name\": file_name,\n",
    "                \"section\": section,\n",
    "                \"page\": page,\n",
    "                \"update_date\": update_date,\n",
    "                \"category\": category\n",
    "            })\n",
    "\n",
    "        # Agr√©gation du contenu des documents\n",
    "        docs_content = \"\\n\\n\".join(doc[\"content\"] for doc in docs_details)\n",
    "\n",
    "        # Formatage des sources avec cat√©gorie, section, page et date\n",
    "        formatted_sources = \"\\n\".join([\n",
    "            f\"[Document: {doc['file_name']}, Cat√©gorie: {doc['category']}, Section: {doc['section']}, Page: {doc['page']}, Mise √† jour: {doc['update_date']}]\"\n",
    "            for doc in docs_details\n",
    "        ])\n",
    "        \n",
    "        # Instructions syst√®me mises √† jour selon les nouvelles exigences\n",
    "        system_instructions = (\n",
    "            \"Tu es un instructeur expert du dispositif KAP Num√©rique. Tu r√©ponds √† des questions en te basant uniquement sur les informations fournies dans le contexte.\\n\\n\"\n",
    "            \n",
    "            \"Consignes de r√©ponse :\\n\"\n",
    "            \"1. Commence ta r√©ponse en r√©p√©tant la question pos√©e, par exemple : 'En r√©ponse √† votre question : \\\"[question]\\\", voici les informations demand√©es :'\\n\"\n",
    "            \"2. Fournis une r√©ponse concise et structur√©e.\\n\"\n",
    "            \"3. Utilise des phrases et des listes √† puces pour organiser les informations.\\n\"\n",
    "            \n",
    "            \"4. Traitement des questions sur un dossier sp√©cifique :\\n\"\n",
    "            \"   - V√©rifie d'abord les informations de la base de donn√©es.\\n\"\n",
    "            \"   - Indique clairement le statut actuel du dossier, la date de derni√®re modification, et les informations pertinentes du demandeur.\\n\"\n",
    "            \"   - Consulte ensuite les documents officiels et les r√®gles pour expliquer les proc√©dures.\\n\"\n",
    "            \"   - Examine les exemples d'√©changes similaires pour adapter le style et le contenu de ta r√©ponse.\\n\"\n",
    "            \n",
    "            \n",
    "            \"5. Traitement des questions g√©n√©rales sur le dispositif KAP Num√©rique :\\n\"\n",
    "            \"   - Consulte en priorit√© les documents officiels puis les r√®gles.\\n\"\n",
    "            \"   - Utilise les exemples d'√©changes pour adapter le format de ta r√©ponse et son niveau de d√©tail.\\n\"\n",
    "            \n",
    "            \"6. Limitations :\\n\"\n",
    "            \"   - Si la question ne concerne ni le dispositif KAP Num√©rique, ni un b√©n√©ficiaire du programme, indique clairement que tu ne peux pas traiter ce type de demande.\\n\"\n",
    "            \"   - Exemple: 'Votre demande ne semble pas concerner le dispositif KAP Num√©rique ou l'un de ses b√©n√©ficiaires. Je ne peux malheureusement pas traiter ce type de requ√™te.'\\n\"\n",
    "            \n",
    "            \"7. Priorisation des sources :\\n\"\n",
    "            \"   - Documents officiels ('officiel') > R√®gles ('regles') > √âchanges ('echanges')\\n\"\n",
    "            \"   - Les informations issues de la base de donn√©es sont prioritaires pour les questions sur un dossier sp√©cifique.\\n\"\n",
    "            \n",
    "            \"8. Cites syst√©matiquement les sources avec le format suivant : [Document: Nom du document, Cat√©gorie: Type de document, Section: Nom de la section, Page: Num√©ro de page, Mise √† jour: Date].\\n\"\n",
    "        )\n",
    "        \n",
    "        # Construction de l'invite utilisateur\n",
    "        user_prompt = f\"Question: {state['question']}\\n\\nContexte extrait des documents et de la base de donn√©es:\\n{docs_content}\"\n",
    "        \n",
    "        # Messages combinant instructions syst√®me et question de l'utilisateur\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Appel au mod√®le LLM avec gestion des erreurs\n",
    "        try:\n",
    "            response = llm.invoke(messages)\n",
    "            return {\"answer\": response.content}\n",
    "        except Exception as e:\n",
    "            return {\"answer\": f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur dans la fonction generate: {e}\")\n",
    "        return{\"answer\":f\"Une erreur s'est produite lors du traitement de votre demande: {e}\"}\n",
    "    \n",
    "    \n",
    "# Construction du graphe d'application\n",
    "def build_graph():\n",
    "    try:\n",
    "        # D√©finir les n≈ìuds du graphe\n",
    "        workflow = StateGraph(State)\n",
    "        \n",
    "        # Ajouter les n≈ìuds individuellement pour mieux contr√¥ler\n",
    "        workflow.add_node(\"search_database\", search_database)\n",
    "        workflow.add_node(\"retrieve\", retrieve)\n",
    "        workflow.add_node(\"generate\", generate)\n",
    "        \n",
    "        # D√©finir les transitions entre les n≈ìuds\n",
    "        workflow.add_edge(START, \"search_database\")\n",
    "        workflow.add_edge(\"search_database\", \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"generate\")\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "        \n",
    "        # Compiler le graphe\n",
    "        return workflow.compile()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la construction du graphe: {e}\")\n",
    "        raise\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tapez 'exit' pour quitter.\n",
      " Connexion r√©ussie avec pymysql.\n",
      " Connexion r√©ussie avec pymysql.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A0278E5A0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A02D16550> server_hostname='api.mistral.ai' timeout=120\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A02ED0950>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:31:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-query-cost', b'64'), (b'ratelimitbysize-limit', b'20000000'), (b'ratelimitbysize-remaining', b'19993607'), (b'x-ratelimitbysize-remaining-month', b'199999915346'), (b'ratelimitbysize-reset', b'11'), (b'x-ratelimitbysize-remaining-minute', b'19993607'), (b'x-ratelimitbysize-limit-minute', b'20000000'), (b'x-ratelimitbysize-limit-month', b'200000000000'), (b'x-envoy-upstream-service-time', b'100'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'123'), (b'x-kong-proxy-latency', b'6'), (b'x-kong-request-id', b'd5345e7b3bb41f11b720a34975753d6c'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b02366ac5d0f9d-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:langchain_core.vectorstores.utils:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A0278EC90>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A02A6F250> server_hostname='api.mistral.ai' timeout=120\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A02DF7410>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Apr 2025 10:32:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'ratelimitbysize-reset', b'9'), (b'ratelimitbysize-limit', b'500000'), (b'ratelimitbysize-query-cost', b'33365'), (b'x-ratelimitbysize-remaining-month', b'999866658'), (b'ratelimitbysize-remaining', b'466635'), (b'x-ratelimitbysize-limit-month', b'1000000000'), (b'x-ratelimitbysize-remaining-minute', b'466635'), (b'x-ratelimitbysize-limit-minute', b'500000'), (b'x-envoy-upstream-service-time', b'14989'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'14989'), (b'x-kong-proxy-latency', b'7'), (b'x-kong-request-id', b'd49fb309efb86948120c8460a3fdbd68'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=zNgvIZzUKTJfhTheXMR24lojURbIHkQyO822ai4xuq8-1743762726-1.0.1.1-yIBCnN8NLONt6Avi6ohLVApcFkrjWhxq6IWBwLcYur45pj3DS3U2_RXPgr2UlxAQFOI0x3JywGBVMaerdUvfMmi5dAJh8UA4nPpYpgx_OFI; path=/; expires=Fri, 04-Apr-25 11:02:06 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'92b02370f8204299-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©ponse : En r√©ponse √† votre question : \"Depuis le 27 F√©vrier 2025, mon dossier a √©t√© mis en paiement. Quand notre association va-t-elle recevoir sa subvention s'il vous plait? Je ne comprends pas trop les deadlines l√†.\", voici les informations demand√©es :\n",
      "\n",
      "- **Statut actuel du dossier 82-2415** : Mandatement\n",
      "- **Date de derni√®re modification** : 27/02/2025\n",
      "- **Informations pertinentes du demandeur** :\n",
      "  - Nom de l'usager : Kassim Riday\n",
      "  - Montant : 3200 ‚Ç¨\n",
      "\n",
      "En r√©ponse √† votre question : \"Quand notre association va-t-elle recevoir sa subvention s'il vous plait?\" :\n",
      "- Le statut de votre dossier est actuellement \"Mandatement\", ce qui signifie que le paiement est en cours de traitement.\n",
      "- Le paiement devrait √™tre effectu√© dans les jours suivants.\n",
      "\n",
      "Pour plus d'informations, vous pouvez contacter l'agent affect√© √† votre dossier : anaelle.nimba@cr-reunion.fr.\n",
      "\n",
      "En vous remerciant de votre compr√©hension.\n",
      "\n",
      "Cordialement,\n",
      "\n",
      "[Document: √âchanges, Cat√©gorie: √âchanges, Section: Dossier 82-2415, Page: N/A, Mise √† jour: N/A]\n"
     ]
    }
   ],
   "source": [
    "# On initialise le graphe\n",
    "graph = build_graph()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Tapez 'exit' pour quitter.\")\n",
    "    \n",
    "    # Tester d'abord la connexion √† la base de donn√©es\n",
    "    try:\n",
    "        db_manager = DatabaseManager()\n",
    "        if not db_manager.tester_connexion():\n",
    "            print(\"Avertissement: Impossible de se connecter √† la base de donn√©es. Les requ√™tes sur les dossiers ne fonctionneront pas.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'initialisation de la connexion √† la base de donn√©es: {e}\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"\\nPosez votre question : \")\n",
    "            if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "                \n",
    "            # Initialiser l'√©tat avec des listes vides pour context et db_results\n",
    "            initial_state = {\n",
    "                \"question\": user_query,\n",
    "                \"context\": [],\n",
    "                \"db_results\": [],\n",
    "                \"answer\": \"\"\n",
    "            }\n",
    "            \n",
    "            # Invoquer le graphe avec gestion d'erreur\n",
    "            try:\n",
    "                state = graph.invoke(initial_state)\n",
    "                print(\"\\nR√©ponse :\", state[\"answer\"])\n",
    "            except Exception as e:\n",
    "                print(f\"\\nErreur lors de l'ex√©cution du graphe: {e}\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nProgramme interrompu par l'utilisateur.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nErreur inattendue: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpython\u001b[49m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39mversion\n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
